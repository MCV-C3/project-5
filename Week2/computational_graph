digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140690918901456 [label="
 (1, 8)" fillcolor=darkolivegreen1]
	140690920923600 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 300)
mat1_sym_strides:       (300, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (300, 8)
mat2_sym_strides:       (1, 300)"]
	140690920923888 -> 140690920923600
	140690918901856 [label="output_layer.bias
 (8)" fillcolor=lightblue]
	140690918901856 -> 140690920923888
	140690920923888 [label=AccumulateGrad]
	140690920923792 -> 140690920923600
	140690920923792 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140690920923744 -> 140690920923792
	140690920923744 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 300)
mat1_sym_strides:       (300, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (300, 300)
mat2_sym_strides:       (1, 300)"]
	140690920924032 -> 140690920923744
	140690918901696 [label="layer2.bias
 (300)" fillcolor=lightblue]
	140690918901696 -> 140690920924032
	140690920924032 [label=AccumulateGrad]
	140690920923984 -> 140690920923744
	140690920923984 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140690920924128 -> 140690920923984
	140690920924128 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (1, 150528)
mat1_sym_strides:             ()
mat2            :           None
mat2_sym_sizes  :  (150528, 300)
mat2_sym_strides:    (1, 150528)"]
	140690920924320 -> 140690920924128
	140691145992336 [label="layer1.bias
 (300)" fillcolor=lightblue]
	140691145992336 -> 140690920924320
	140690920924320 [label=AccumulateGrad]
	140690920924272 -> 140690920924128
	140690920924272 [label=TBackward0]
	140690920924368 -> 140690920924272
	140691145985776 [label="layer1.weight
 (300, 150528)" fillcolor=lightblue]
	140691145985776 -> 140690920924368
	140690920924368 [label=AccumulateGrad]
	140690920923696 -> 140690920923744
	140690920923696 [label=TBackward0]
	140690920924416 -> 140690920923696
	140690918901536 [label="layer2.weight
 (300, 300)" fillcolor=lightblue]
	140690918901536 -> 140690920924416
	140690920924416 [label=AccumulateGrad]
	140690920923840 -> 140690920923600
	140690920923840 [label=TBackward0]
	140690920924224 -> 140690920923840
	140690918901616 [label="output_layer.weight
 (8, 300)" fillcolor=lightblue]
	140690918901616 -> 140690920924224
	140690920924224 [label=AccumulateGrad]
	140690920923600 -> 140690918901456
}
